---
title: "breast cancer analysis"
author: "Yifei Liu"
date: "4/20/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=F, warning=F}
library(tidyverse)
library(broom)
library(scales)
library(MASS)
library(ggcorrplot)
library(skimr)
library(tidymodels)
library(parsnip)
library(yardstick)
library(modelr)
library(Metrics)
library(ranger)
library(rsample)
library(yardstick)
library(dplyr)
library(ranger)
theme_set(theme_minimal())
```

load and clean the dataset. 

## EDA

try to visualze the dataset we have, our objective,

1. see the corr between x variables
2. see y variable compaision
3. how many missing value do we have
4. what type of prediction do we need to apply. e.g. lm, glm, or KNN. 



```{r}
breast_cancer <- read_csv("https://raw.githubusercontent.com/ECE-GitHub/breast-cancer-detection/master/data/breast-cancer-wisconsin.txt", na = c(NA, "?", "No idea", "#"))

breast_cancer <- breast_cancer %>%
  setNames(str_replace_all(names(breast_cancer), " ", "_")) %>%
  filter(Class %in% c(2,4)) %>%
  mutate(Class = as.factor(Class)) %>%
  select(-Index, - ID) %>%
  mutate(Class = case_when(
    Class == 2 ~ "benign",
    T ~ "malignant"
  ),
  Class = as.factor(Class))

  
```


the objective of this analysis is to help doctor to diagnose breast cancer, to be specific, to classific whether the result is benign or malignant, classification problem, now we can use several method in statistical learning, in supervisousn learning, such as logistical regress, lm un-supervise learning such as KNN and etc.

But first we need to visualize our data first. 

let's take a look at the y variable

```{r}
breast_cancer %>%
  count(Class) %>%
  ggplot(aes(Class, n, fill = Class)) +
  geom_col() +
  labs(title = "Number of people diagnose with breast cancer",
       x = "", y = "", legend = "Class")
  


```

we can see in this the number of malignant is significant higher than benign (is there a label mistake? I thought most case should be more benign than mailgnant.). In this case, we can use confusion matrices to present our result. 

Next let's take a look at x variables. see quantile, how many NA do we have for all the dataset. 

```{r}
breast_cancer %>%
  skim(Clump_Thickness:Mitoses, Class) %>%
  pander()

breast_cancer[!complete.cases(breast_cancer),]

breast_cancer <- breast_cancer %>%
  na.omit()

```
We can do futuer study, like ask question about why all the observations that have incomplete are missing Bare_Nuclei value. 

Since you have abundant of data, and only hand full of missing data, we can just filter those data out. 


let see the relationship between x variable and y variable

```{r}

breast_cancer %>%
  ggplot(aes(Class, Clump_Thickness)) +
  geom_boxplot(aes(fill = Class), alpha = 0.25) + 
  geom_jitter(alpha = 0.5, height = 0.2, width = 0.25, aes(color = Class)) +
  scale_color_manual(values = c("blue", "red")) +
  scale_fill_manual(values = c("blue", "red")) +
  labs(title = "Clump Thickness difference between Benign and mailgnant",
       x = "", y = "", subtitle = "Score between 1:10")

```

Since we have 9 x variable and one one y variable, we can use density plot show the relationship between x and y variables. 

```{r}
breast_cancer %>%
  gather(key = "key", value = "value", -Class) %>%
  na.omit() %>%
  ggplot(aes(value, fill = as.factor(Class))) +
  geom_density() +
  facet_wrap(~ key) +
  labs(fill = "Class")

breast_cancer %>%
  gather(-Class, value = "value", key = "key") %>%
  ggplot(aes(value, fill = Class)) +
  geom_density(position = "fill", alpha = 0.25) +
  facet_wrap(~ key) +
  scale_fill_manual(values = c("blue", "red"))

```

we can see in this density chart, beside single Epithelial Cell size and Mitosens, all other chart yield similary result, most of these start to show class as malignanat as those value increase. We can use correlation plot to identify how many of these variables have a relative higher correlationship. 

let's visualzie the correlationship between all those x variables.

```{r}
corr <- breast_cancer %>%
  select(-Class) %>%
  setNames(str_replace_all(names(breast_cancer)[-10], "_", " ")) %>%
  na.omit() %>%
  cor() %>%
  round(digits = 2)


corr %>%
  ggcorrplot(hc.order = T, type = "lower",
             outline.color = "white",
             colors = c("#6D9EC1", "white", "#E46726"),
             lab = T) 


```



let's visualize the x variable distribution relative to y value, since all the x variable scale from 1:10

```{r}
breast_cancer %>%
  gather(-Class, key = "key", value = "value") %>%
  mutate(key = str_replace_all(key, "_", " ")) %>%
  ggplot(aes(fct_reorder(key, value, .fun = median), value, fill = key)) +
  geom_boxplot(alpha = 0.25, show.legend = F) +
  facet_wrap(~ Class) +
  coord_flip() +
  labs(x = "", y = "",
       title = "Value of feature that seperate benign cells and malignant cells",
       subtitle = "The ratio of Malignant:Benign = 33:1")

```

we can use bootstrap to gague the true median or mean of each variables. 

```{r}
geom_mean <- function(x) {
  exp(mean(log(x + 1)) - 1)
}

breast_cancer %>%
  gather(key, value, -Class) %>%
  bootstraps(times = 25) %>%
  unnest(map(splits, as.data.frame)) %>%
  group_by(id, key, Class) %>%
  summarize(avg_mean = geom_mean(value)) %>%
  ungroup() %>%
  mutate(key = fct_reorder(key, avg_mean)) %>%
  ggplot(aes(key, avg_mean, group = key)) +
  geom_boxplot() +
  coord_flip() +
  facet_wrap(~ Class, scales = "free_x") +
  labs(x = "Mean Score",
       y = "",
       title = "")

```





Conclusion

1. we have 9 x variables, couple of those varibale have relative high correlationship such as, uniformity of cell size, unifority of cell shape and normal nucleoli and single epithelial cell size and clump thickness and marginal adhesion.

2. there are some missing value, but only around 16. So we can just filter the data out, you can ask adviors how to deal with it. In my case, I just filter out the observations that contain missing values. 



## Modeling

Most important question need to answer when building the model 

1. How well my model well perform on the new data
2. Did I select the best performing model 

### split dataset
80/20 split ratio

```{r}
set.seed(2019)

cancer_split <- initial_split(breast_cancer, prop = 0.8)

cancer_train <- training(cancer_split)
cancer_test <- testing(cancer_split)

```

using training data exclusively to train my model

use cross validation, benefit,

1. this well enable user to use all the training dataset to evaluate overall performance of the model
2. able to calcuate the multiple measure of the performance. This count the nature variation of the performance of the model.

```{r}
cv_split <- cancer_train %>%
  vfold_cv(v = 20)
  
cv_split$splits[[1]]

cv_data <- cv_split %>%
  mutate(train = map(splits, ~training(.x)),
         validate = map(splits, ~testing(.x)))
```



### first model - logistic regression

```{r}
cv_models_lr <- cv_data %>%
  mutate(model = map(train, ~glm(Class ~., data = .x, family = "binomial")))

cv_prep_lr <- cv_models_lr %>%
  mutate(validate_actual = map(validate, ~.x$Class == "malignant"),
         validate_predicted = map2(model, validate, ~predict(.x, .y ,type = "response") > 0.5))

cv_eval_lr <- cv_prep_lr %>%
  mutate(validate_recall = map2_dbl(validate_actual, validate_predicted, ~ recall(actual = .x, predicted = .y)),
         validate_accuracy = map2_dbl(validate_actual, validate_predicted, ~ accuracy(actual = .x, predicted = .y)),
         validate_precision = map2_dbl(validate_actual, validate_predicted, ~ precision(actual = .x, predicted = .y)))

cv_eval_lr %>%
  select(validate_recall, validate_accuracy, validate_precision) %>%
  setNames(c("recall", "accuracy", "precision")) %>%
  gather() %>%
  ggplot(aes(key, value, fill = key)) +
  geom_boxplot(show.legend = F) +
  scale_y_continuous(labels = scales::percent_format()) +
  expand_limits(y = 0.995) +
  labs(x = "", y = "",
       title = "Logisticl regression accuracy, precision, recall values")

```

we can see the distribution of all these modl are quite stable. 


### second model: random forecast

```{r}
cv_tune <- cv_data %>%
  crossing(mtry = c(3,5,7,9))

# I only tune one parameter in this case
cv_models_rf <- cv_tune %>%
  mutate(model = map2(train,mtry, ~ranger(Class ~., data = .x, mtry = .y, num.trees = 100,
                                           seed = 2019)))

cv_prep_rf <- cv_models_rf %>%
  mutate(validate_actual = map(validate, ~.x$Class == "malignant"),
         validate_predicted = map2(model, validate, ~predict(.x, .y ,type = "response")$predictions == "malignant"))

cv_eval_rf <- cv_prep_rf %>%
  mutate(validate_recall = map2_dbl(validate_actual, validate_predicted, ~ recall(actual = .x, predicted = .y)),
         validate_accuracy = map2_dbl(validate_actual, validate_predicted, ~ accuracy(actual = .x, predicted = .y)),
         validate_precision = map2_dbl(validate_actual, validate_predicted, ~ precision(actual = .x, predicted = .y)))

cv_eval_rf %>%
  select(validate_recall, validate_accuracy, validate_precision, mtry) %>%
  setNames(c("recall", "accuracy", "precision", "mtry")) %>%
  gather(key, value, -mtry) %>%
  ggplot(aes(key, value, fill = key)) +
  geom_boxplot(show.legend = F) +
  scale_y_continuous(labels = scales::percent_format()) +
  facet_wrap(~mtry) +
  expand_limits(y = 0.995) +
  labs(x = "", y = "",
       title = "Random Forecast model",
       subtitle = "differnt mtry")
  

```

### thrid model - linear discrimate analysis

```{r}
cv_models_lda <- cv_data %>%
  mutate(model = map(train, ~lda(Class ~., data = .x)))

cv_prep_lda <- cv_models_lda %>%
  mutate(validate_actual = map(validate, ~.x$Class == "malignant"),
         validate_predicted = map2(model, validate, ~predict(.x, .y ,type = "response")$class == "malignant"))

cv_eval_lda <- cv_prep_lda %>%
  mutate(validate_recall = map2_dbl(validate_actual, validate_predicted, ~ recall(actual = .x, predicted = .y)),
         validate_accuracy = map2_dbl(validate_actual, validate_predicted, ~ accuracy(actual = .x, predicted = .y)),
         validate_precision = map2_dbl(validate_actual, validate_predicted, ~ precision(actual = .x, predicted = .y)))

cv_eval_lda %>%
  dplyr::select(validate_recall, validate_accuracy, validate_precision) %>%
  setNames(c("recall", "accuracy", "precision")) %>%
  gather() %>%
  ggplot(aes(key, value, fill = key)) +
  geom_boxplot(show.legend = F) +
  scale_y_continuous(labels = scales::percent_format()) +
  expand_limits(y = 0.995) +
  labs(x = "", y = "",
       title = "Linear Discriminant Analysis accuracy, precision, recall values")

```



base on this two model we can say the accuacy, precision and recall are quite stable for models. So I use the logisticl regression model as final model to predict the test dataset. Because it's relartive easy to interpret. 


Let's use 


```{r}
test_actual <- cancer_test$Class == "malignant"

best_model <- glm(Class ~., data = cancer_train, family = "binomial")

test_predicted <- predict(best_model, cancer_test, type = "response") > 0.5

table(test_actual, test_predicted) %>%
  knitr::kable()


precision(test_actual, test_predicted)
accuracy(test_actual, test_predicted)
recall(test_actual, test_predicted)
```


Overall accurate rate is pretty good. I will use this model in practice to help doctor to predict whether a cell is malignant base on all the predictors. 
















