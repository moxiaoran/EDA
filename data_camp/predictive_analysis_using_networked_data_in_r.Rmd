---
title: "Predictive Analytics using Networked Data in R"
author: "Yifei Liu"
date: "11/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

load pacakges and data
```{r}
library(tidyverse)
library(igraph)
library(ggraph)
library(corrplot)
library(pROC)
library(visNetwork)
theme_set(theme_minimal())

detach("package:dplyr", unload = TRUE)
library(dplyr)

load("/Users/yifeiliu/Documents/R/data/EDA/datacamp/network/StudentEdgelist.RData")
load("/Users/yifeiliu/Documents/R/data/EDA/datacamp/network/StudentCustomers.RData")
load("/Users/yifeiliu/Documents/R/data/EDA/datacamp/network/StudentNetwork.RData")

```


```{r}

vertex_attr(g1)

sapply(ego(g1,1,V(g1),mode = 'all',mindist = 1), function(v) sum(V(g1)[v]$networkChurn == "Yes"))

```



## Introduction, networks and labelled networks

### Introduction

### Most likely to churn
Below you see a network with three type of nodes:

- Non-churners (white)
- Churners (red)
- Unknown label (blue)

Under the assumption that churn is a social phenomenon, which of the blue nodes is most likely to churn? 

![](https://assets.datacamp.com/production/repositories/2073/datasets/752c77509d5ddb2dd3ea8b39118aad3a1eb84f68/Screen%20Shot%202019-08-29%20at%203.07.10%20PM.png)

Answer: C is connected to the most churners, three in total.


### Create a network from an edgelist

In this exercise, you will create a network from an edgelist.

You will create an igraph object from data stored in an edgelist using the function graph_from_data_frame(). The data is a social network of customers and each row of the edgeList dataframe represents an edge in the network. The edges in this network are undirected and have weight 1. This is indicated by the directed argument of the function, which is logical so the value FALSE means that the network is undirected.

The igraph package has been loaded for you.


```{r}

head(edgeList)

# Construct the igraph object
network <- graph_from_data_frame(edgeList, directed = F)

# View your igraph object
network

```


### Labeled networks, Social influence

### Labeling nodes

In this exercise, you will label the nodes in the network. You are given a dataframe called customers with the same customer IDs as in the network. Each customer has an indication of whether they churned or not, given by 1 or 0 respectively. You will add the churn status to the nodes of the network and visualize it.

Note that a network can have both node and edge attributes. The node attributes are represented by the function V() (for vertex) and the edge attributes by the function E().
The node attributes of the churn network are V(network).

```{r}

# Inspect the customers dataframe
head(customers)

# Count the number of churners and non-churners
table(customers$churn)

# Add a node attribute called churn
V(network)$churn <- customers$churn

# Visualize the network
plot(network, vertex.label = NA, edge.label = NA,
    edge.color = 'black', vertex.size = 2)

## use ggplot to vis the network

ggraph(network, layout = "with_kk") +
  geom_edge_link() +
  geom_node_point(aes(color = factor(churn))) 
  

```

### Coloring nodes

In this exercise, you will color the nodes in the network depending on the churn status. The customers who have churned will be colored red and the non-churners will be colored white. Then you will visualize the network again.

```{r}

# Add a node attribute called color
V(network)$color <- V(network)$churn

# Change the color of churners to red and non-churners to white
V(network)$color <- gsub("1", "red", V(network)$color) 
V(network)$color <- gsub("0", "white", V(network)$color)

# Plot the network
plot(network, vertex.label = NA, edge.label = NA,
    edge.color = "black", vertex.size = 2)


```

### Visualizing Churners

In the previous exercises, you noticed that it can be difficult to visualize a large network. Now you will visualize only the churners in the network by conditioning on the churn attribute.

- Create a new igraph object called churnerNetwork.
  - Use the function induced_subgraph() to generate a subgraph of the already existing network.
  - Select the nodes that have the node attribute churn equal to 1.
- Visualize churnerNetwork using plot().


```{r}

# Create a subgroup with only churns
churnerNetwork <- induced_subgraph(network,
                                 v = V(network)[which(V(network)$churn == 1)])

# plot the churner network
plot(churnerNetwork, vertex.label = NA, vertex.size = 2)

# ggplot the churner network
ggraph(churnerNetwork, layout = "in_circle") +
  geom_node_point(aes(color = color)) +
  geom_edge_link(alpha = 0.5)

```


### Relational Neighbor Classifier

In this exercise, you will apply a simple network based classifier called the relational neighbor classifier. It uses the class labels of neighboring nodes to compute a churn probability for each node in the network.
For example, in the network below where red nodes denote churners and white nodes denote non-churners, the churn probability of the blue node is 0.4.


![](https://assets.datacamp.com/production/repositories/2073/datasets/ae7dc5c4fda0009d30bb7090903127ebd6bbdbf2/RL1.jpg)

You are given two vectors: ChurnNeighbors and NonChurnNeighbors with each customer's number of neighbors that have churned and not churned, respectively.



```{r}

# # Compute the churn probabilities
ChurnNeighbors <- map_int(ego(network, 1, V(network), mindist = 1), function(v) sum(V(network)[v]$churn == 1))
NonChurnNeighbors <- map_int(ego(network, 1, V(network), mindist = 1), function(v) sum(V(network)[v]$churn == 0))

churnProb <- ChurnNeighbors / (ChurnNeighbors + NonChurnNeighbors)

# Find who is most likely to churn
mostLikelyChurners <- which(churnProb == max(churnProb))

# Extract the IDs of the most likely churners
customers$id[mostLikelyChurners]

map_int(ego(network, 1, V(network), mindist = 1), function(v) sum(V(network)[v]$churn == 1))
```

Reference: [FAST way to sum up neighbors' attributes in a large graph in R](https://stackoverflow.com/questions/39172602/fast-way-to-sum-up-neighbors-attributes-in-a-large-graph-in-r)

### Challenges

### Challenges in Network learning
Which statement is wrong about the challenges of social network based inference?

- Splitting the data into training and test sets is not easy.

- Inferences about entities might affect each other.

- The data is independent and identically distributed.

- The properties of connected nodes might be correlated.

Answer: C is incorrect

The data is not independent and identically distributed (iid) as the label of one node might influence the label of a related node and there is correlational behavior between nodes.

### Probabilistic Relational Neighbor Classifier

In this exercise, you will apply the probabilistic relational neighbor classifier to infer churn probabilities based on the prior churn probability of the other nodes.

Instead of knowing the nodes' labels, suppose you know each node's probability of churn, as in the image below. In the image, C stands for churn and NC for non-churn. Then, as before, you can update the churn probability of the nodes by finding the average of the neighboring nodes' churn probabilities.

![](https://assets.datacamp.com/production/repositories/2073/datasets/46e37bfad18a72bb4d6296b67dc5a0d2d6584edd/bRL2.jpg)

Don't really understand
```{r}
# Find churn probability of the 44th customer
churnProb[44]

AdjacencyMatrix <- as_adjacency_matrix(network)
neighbors <- ego_size(network)

# Update the churn probabilties and the non-churn probabilities
churnProb_updated <- as.vector((AdjacencyMatrix %*% churnProb) / neighbors)

# Find updated churn probability of the 44th customer
churnProb_updated[44]

```


applying the probabilistic relational neighbor classifier! The churn probabiltiy of the 44th customer decreased from 0.25 to 0.173

### Collective Inferencing

Collective inferencing is a procedure to simultaneously label nodes in interconnected data to reduce classification error.

In this exercise you will perform collective inferencing and see the effect it has on the churn prediction using the AUC performance measure. AUC, or area under the ROC curve, is commonly used to assess the performance of classification techniques.

- AUC = probability that a randomly chosen churner is ranked higher by the model than a randomly chosen non-churner
- AUC = number between 0.5 and 1, where a higher number means a better model
Does collective inferencing increase the AUC value?

-Compute the AUC of the relational neighbor classifier by calling the auc function in the pROC package, using the actual churn labels customers$churn and the churnProb as the predicted value.
- Write a for loop where you apply the probabilistic relational neighbor classifier ten times, and assign the - value again to the churnProb vector in each iteration.
Compute the AUC again using the updated churnProb vector.

```{r}

# Compute the AUC
auc(customers$churn, as.vector(churnProb))

# Write a for loop to update the probabilities
for(i in 1:10){
 churnProb <- as.vector((AdjacencyMatrix %*% churnProb) / neighbors)
}

# Compute the AUC again
auc(customers$churn, as.vector(churnProb))

```

The AUC decrease from 0.5215 to 0.5054!

## Homophily

### Homophily

### Homophilic networks
In which network would you expect to observe the least homophily?

Network of fraudulent and legitimate credit card transactions

Age in a network of Facebook friends

Height of researchers in a collaboration network

All of the above

Answer: C. Height in this case is more randomly distributed

### Extracting types of edges

In this exercise, you will match the customer IDs in the customer dataframe with the customer edgelist to find out whether each edge is a churn, non-churn or a mixed edge. Using the function match(), you will add two columns to the edgelist.

- fromLabel with the churn status of the from column
- toLabel with the churn status of the to column

The command match(x, y) returns a vector with the location of x in y. In the figure above match(edgeList\$from, customers\$id) is 1,1,1,2,2. For example, the fourth line in edgeList\$from, which is customer with id 393, is the second element in customers\$id. The churn label of this customer is, therefore, customers[2,2] or 0. Similarly, the churn label of everyone in edgeList\$from is customers[match(edgeList\$from, customers$id),2].

- Add a column called FromLabel to the edgeList dataframe with the label of the from nodes by matching customers\$id with edgeList\$from and extracting customers\$churn.
- Do the same for the to edges, and call this column ToLabel.
- Add a column called edgeType to the edgeList dataframe that is the sum of the columns FromLabel and ToLabel.
- Use the table() function to see the number of each type of edge.


```{r}

# Add the column edgeList$FromLabel
edgeList$FromLabel <- customers[match(edgeList$from, customers$id), 2]
 
# Add the column edgeList$ToLabel
edgeList$ToLabel <- customers[match(edgeList$to, customers$id), 2]
 
# Add the column edgeList$edgeType
edgeList$edgeType <- edgeList$FromLabel + edgeList$ToLabel
 
# Count the number of each type of edge
table(edgeList$edgeType)


```

### Counting types of edges

In this exercise, you will count the number of each edge type using the edgeType column in the edgeList dataframe. In the edgeList$edgeType column, there are three different values:

- 0 for edges that connect two non-churn nodes.
- 1 for edges that connect a non-churn and a churn node. These are called mixed or cross-label edges.
- 2 for edges that connect two churn nodes.

- Count the number of churn edges by conditioning on edgeList$edgeType. Assign the value to ChurnEdges.
- Count the number of non-churn edges by conditioning on edgeList$edgeType. Assign the value to NonChurnEdges.
- Count the number of mixed edges by conditioning on edgeList$edgeType. Assign the value to MixedEdges.
- Count the total number of edges and assign the value to edges.

```{r}
# Count churn edges
ChurnEdges <- sum(edgeList$edgeType == 2)
 
# Count non-churn edges
NonChurnEdges <- sum(edgeList$edgeType == 0)
 
# Count mixed edges
MixedEdges <- sum(edgeList$edgeType == 1)
 
# Count all edges
edges <- ChurnEdges + NonChurnEdges + MixedEdges

#Print the number of edges
edges
```

### Counting nodes and computing connectance

In this exercise, you will count the number of each type of node using the customers dataframe. The churn column has two different values:

0 for non-churners
1 for churners

You will also compute the network's connectance using the formula $p = \frac{2E}{N(N-1)}$where N is the number of nodes and E the number of edges in the network.

- Count the number of churn nodes by conditioning on customers$churn.
- Count the number of non-churn nodes by conditioning on customers$churn.
- Count the total number of nodes and name the variable nodes.
- Compute the network's connectance using the formula for p shown above. You can use edges from the previous exercise.


```{r}
# Count the number of churn nodes
ChurnNodes <- sum(customers$churn == 1)
 
# Count the number of non-churn nodes
NonChurnNodes <- sum(customers$churn == 0)
 
# Count the total number of nodes
nodes <- ChurnNodes + NonChurnNodes
 
# Compute the network connectance
connectance <- 2 * edges / nodes / (nodes - 1)

# Print the value
connectance

```

### Dyadicity

### Same label edges
How many edges connect two black nodes in this network?

![](https://assets.datacamp.com/production/repositories/2073/datasets/3f3a5849aa4a5c8248d7ceefa9d47205e07aff2f/Ch2EX2_1.jpg)

Answer: 2

### Dyadicity of churners

In this exercise, you will compute the dyadicity amongst the churners in the network to see if they share more or fewer edges than expected in a random configuration of the network.

The variables ChurnNodes, ChurnEdges, and connectance are available for you to use.

For expected dyadicity, use the formula $\frac{n_c(n_c - 1)}{2}* p $ Where $n_c$ is the number of churners, N
 is the number of nodes, and p is the connectance. Dyadicity of the churners is the ratio between the actual churners and the expected churn dyadicity.

- Compute the expected dyadicity of churners and assign it to the variable ExpectedDyadChurn.
- Compute the dyadicity of the churners by dividing ChurnEdges with ExpectedDyadChurn. Call this value DyadChurn.
- Inspect DyadChurn.


```{r}

# Compute the expected churn dyadicity
ExpectedDyadChurn = ChurnNodes * (ChurnNodes - 1) /2 * connectance

# Compute the churn dyadicity
DyadChurn <- ChurnEdges / ExpectedDyadChurn
 
# Inspect the value
DyadChurn

```

computing the dyadicity! Because the value is greater than 1, there is dyadicity amonst the churners!

### Dyadicity of non-churners

In the last exercise, you computed the dyadicity of churners. Now you will compute the dyadicity of non-churners in the same way.

Given the variables NonChurnNodes, NonChurnEdges and connectance, what is the dyadicity of the non-churners?

Remember the formula $\frac{n_{NC} (n_{NC} - 1)}{2}* p$ p is connectance

```{r}

ExpectedDyadNonChurn <- NonChurnNodes * (NonChurnNodes - 1) * connectance / 2

DyadNonChurn <- NonChurnEdges / ExpectedDyadNonChurn

DyadNonChurn

```

### Heterophilicity

### Cross label edges
How many cross label edges does this network have?

![](https://assets.datacamp.com/production/repositories/2073/datasets/3f3a5849aa4a5c8248d7ceefa9d47205e07aff2f/Ch2EX2_1.jpg)

Answer: 7

### Compute heterophilicity

Similar to dyadicity, heterophilicity is the ratio between the actual number of mixed label edges and the expected number of mixed label edges. In this exercise, you will compute the network's heterophilicity.

For the expected number of mixed edges, use the formula $n_{NC} * n_{c} * p$ where $n_{NC}$ is the number of non-churners, $n_c$ is the number of churners and p is the connectance.

You can use the variables NonChurnNodes, ChurnNodes, connectance, and MixedEdges.

- Compute the expected number of mixed edges and assign it to the variable ExpectedHet.
- Compute the heterophilicity by dividing the actual number of mixed edges with the expected number of mixed edges. Call this variable Het.
- Is the network heterophilic? Inspect Het to find out.


```{r}

# comute the expected heterophilicity
ExpectedHet <- ChurnNodes * NonChurnNodes * connectance

# Comute the heterophilicity
Het <- MixedEdges / ExpectedHet

# Inspect the heterophilicity
Het


```
computing the heterophilicity! It is less than one, which means that the network is heterophobic.

### Summary of homophily

### Dyadicity, Heterophilicity, & Homophily
Depending on the values of H (heterophilicity) and D (diadicity), when is a network homophilic?

H < 1 & D > 1


### Is the network homophilic?

Look at the network below. It has 35 nodes and 34 edges, with 9 green nodes, 2 edges connecting two green nodes, and 15 mixed edges. The connectance is p=0.057
. This value is stored in the variable p.

From the perspective of the green nodes, is the network homophilic? 

## Network Featurization

### Basic Network features

### Simple network features

In this exercise, you will extract simple network features from the churn network and add them to the network object as node attributes. The igraph object called network with the node attribute Churn, indicating customers who have already churned, has been pre-loaded. The igraph library has also been loaded for you.

1. Extract the normalized degree of each node in the network using the degree() function and name it degree

```{r}
# Extract network degree
V(network)$degree <- degree(network, normalized=TRUE)

```

2. Use neighborhood.size() to extract the size of the second order neighborhood and call it degree2. Normalize degree2.

```{r}
# Extract 2.order network degree
degree2 <- neighborhood.size(network, order = 2)

# Normalize 2.order network degree
V(network)$degree2 <- degree2 / (length(V(network)) - 1)

```

3. Extract the number of triangles with count_triangles() and add to network as a node attribute called triangles.

```{r}
# Extract number of triangles
V(network)$triangles <- count_triangles(network)

```

### Centrality features

In this exercise, you will compute and extract the centrality features betweenness, closeness, and eigenvector centrality.

1. Extract the normalized betweenness of the nodes using betweenness(). Add it as a node attribute called betweenness.

```{r}
# Extract the betweenness
V(network)$betweenness <- betweenness(network, normalized=TRUE)
```

2. Extract the normalized closeness using closeness() and add it to network as a node attribute called closeness

```{r}
# Extract the closeness
V(network)$closeness <- closeness(network, normalized=TRUE)

```

3. Extract the scaled eigenvector centrality using eigen_centrality() and add its vector attribute as eigenCentrality.

```{r}
# Extract the eigenvector centrality
V(network)$eigenCentrality <- eigen_centrality(network, scale = TRUE)$vector
```

### Transitivity

In this exercise you will compute the network's transitivity, or clustering coefficient, both on the node level and on the global level.

- Extract the transitivity of each node in the network using the transitivity() function and add it to the network object as a node attribute called transitivity.
- Make sure you use the type local.
- To avoid NA values, specify that isolated nodes should get the value zero.
- Compute the global transitivity of the network using the function transitivity().

```{r}
# Extract the local transitivity
V(network)$transitivity <- transitivity(network, type="local", isolates='zero')

# Compute the network's transitivity
transitivity(network)
```


### Link-Based Features

### Adjacency matrices

In this exercise, you will extract and compute the first and second order adjacency matrices of the network.
You've already seen how to extract the first order adjacency matrix using the as_adjaceny_matrix() function in the slides. For the second-order adjacency matrix, you need to multiply the first order matrix with itself and replace all the positive values with 1 since we are working with unweighted networks only. You also need to make sure the elements on the diagonal are 0 since we do not allow self-edges.

- Extract the network's adjacency matrix using the as_adjacency_matrix() function. Name the matrix AdjacencyMatrix.
- Compute the second order adjacency matrix by multiplying AdjacencyMatrix with itself and call it SecondOrderMatrix_adj.
- Create a new matrix, SecondOrderMatrix, by conditioning on SecondOrderMatrix_adj to make all positive values equal to 1. The elements on the diagonal should be 0.
- Inspect the first 10 rows and first 10 columns of SecondOrderMatrix.

```{r}
# Extract the adjacency matrix
AdjacencyMatrix <- as_adjacency_matrix(network)

# Compute the second order matrix
SecondOrderMatrix_adj <- AdjacencyMatrix %*% AdjacencyMatrix

# Adjust the second order matrix
SecondOrderMatrix <- ((SecondOrderMatrix_adj) > 0) + 0
diag(SecondOrderMatrix) <- 0

# Inspect the second order matrix
SecondOrderMatrix[1:10, 1:10]

```

### Link-based features

In this exercise, you will compute first order link-based features by multiplying the Churn attribute of the network with the network's adjacency matrix.

Note, that since churn is a binary indicator, the attribute Churn has 1 for churners and 0 for non-churners. Consequently, the attribute 1-Churn has 1 for non-churners and 0 for churners. This is helpful when computing the number of non-churn neighbors.

- Compute the attribute ChurnNeighbors, i.e. the number of neighbors who churned, by multiplying AdjacencyMatrix with the Churn attribute of network. Apply as.vector() to the result and add it to the network.
- Similarly, compute NonChurnNeighbors, i.e. the number of non-churn neighbors.
- Calculate the attribute RelationalNeighbor, the ratio of churners in the neighborhood, by dividing ChurnNeighbors with the sum of ChurnNeighbors and NonChurnNeighbors.

```{r}

# Compute the number of churn neighbors
V(network)$ChurnNeighbors <- as.vector(AdjacencyMatrix %*% V(network)$churn)

# Compute the number of non-churn neighbors
V(network)$NonChurnNeighbors <- as.vector(AdjacencyMatrix %*% (1 - V(network)$churn))

# Compute the relational neighbor probability
V(network)$RelationalNeighbor <- as.vector(V(network)$ChurnNeighbors / (V(network)$ChurnNeighbors +
                                             V(network)$NonChurnNeighbors))

```


### Second order link-based features

In this exercise, you will compute the number and ratio of churn and non-churn neighbors in the second order neighborhood. The procedure is the same as in the previous exercise, except now you use the second order adjacency matrix.

- Compute the number of churn neighbors in the second order neighborhood using SecondOrderMatrix and the Churn attribute. Convert the result with as.vector() and add it as ChurnNeighbors2 to network.
- Also compute NonChurnNeighbors2, the number of non-churn neighbors in the second order neighborhood.
- Calculate RelationalNeighbor2, the ratio of churners in the second order neighborhood, by dividing ChurnNeighbors2 with the sum of ChurnNeighbors2 and NonChurnNeighbors2.

```{r}

# Compute the number of churners in the second order neighborhood
V(network)$ChurnNeighbors2 <- as.vector(SecondOrderMatrix %*% V(network)$churn)

# Compute the number of non-churners in the second order neighborhood
V(network)$NonChurnNeighbors2 <- as.vector(SecondOrderMatrix %*% (1 - V(network)$churn))

# Compute the relational neighbor probability in the second order neighborhood
V(network)$RelationalNeighbor2 <- as.vector(V(network)$ChurnNeighbors2 / 
    (V(network)$ChurnNeighbors2 + V(network)$NonChurnNeighbors2))

```


### Neighborhood link-based features

Sometimes, the feature values of neighboring nodes have an effect on behavior. In this exercise, you will look at the attribute value of neighboring nodes and compute their average. You will do this for degree, triangles, transitivity, and betweenness.

You need to:

- Multiply the adjacency matrix with the network attribute you want to find the average of, to obtain the overall value in the neighborhood.
- To get the average, divide by the node's degree, given by the vector degree which has been pre-loaded.
- Finally, convert the result to a vector and assign to network as a node attribute.

1. Compute the neighborhoods' average degree using the degree node attribute of network and name it averageDegree.

```{r}
degree <- degree(network)

# check network nodes attr names
vertex_attr_names(network)

# Extract the average degree of neighboring nodes
V(network)$averageDegree <-  as.vector(AdjacencyMatrix %*% V(network)$degree) / degree

```

2. Use the triangles node attribute to compute averageTriangles, just as you computed averageDegree with degree.

```{r}

# Extract the average number of triangles of neighboring nodes
V(network)$averageTriangles <- 
    as.vector(AdjacencyMatrix %*% V(network)$triangles) / degree

```

3. Compute averageTransitivity using the node attribute transitivity.

```{r}

# Extract the average transitivity of neighboring nodes    
V(network)$averageTransitivity<-
    as.vector(AdjacencyMatrix %*% V(network)$transitivity) / degree

```

4. Use the node attribute betweenness to compute averageBetweenness.

```{r}

# Extract the average betweenness of neighboring nodes    
V(network)$averageBetweenness <- 
    as.vector(AdjacencyMatrix %*% V(network)$betweenness) / degree

```

### PageRank

### Most influential node
Below you see the kite network. Which of these people is most influential?

![](https://assets.datacamp.com/production/repositories/2073/datasets/78d5dab1e700608bd32cc3f9d1a41115c55ca59b/Kite.png)

### Changes in PageRank

The PageRank formula $\vec{PR} = \alpha * A * \vec{PR} + (1-\alpha)*\vec{e}$ can be solved for $\vec{PR}$ iteratively. In each iteration, the current value of $\vec{PR}$ is used to compute a new value that is closer to the true value. This means that the difference between the $\vec{PR}_s$ of every two subsequent iterations becomes smaller and smaller until $\vec{PR}$ converges to the true value and the difference becomes (almost) zero. In this exercise, you will inspect the PageRank algorithm and how it converges.

- Compute one iteration with the PageRank algorithm using page.rank() with network and indicating niter=1. Extract the vector attribute and assign the result to iter1.
- Repeat the last step with niter=2. Assign the result to iter2.
- Compute the sum of the absolute difference between the vectors iter1 and iter2.
- We have computed iter9 and iter10 in the same way as iter1 and iter2. Is the difference between these two iterations less than between iterations 1 and 2.


```{r}
# Compute one iteration of PageRank 
iter1 <- page_rank(network, options = list(niter = 1))$vector

# Compute two iterations of PageRank 
iter2 <- page.rank(network, options = list(niter = 2))$vector

# Inspect the change between one and two iterations
sum(abs(iter2 - iter1))

# Compute nine and ten iterations of PageRank 
iter10 <- page.rank(network, options = list(niter = 10))$vector
iter9 <- page.rank(network, options = list(niter = 9))$vector

# Inspect the change between nine and ten iterations
sum(abs(iter10 - iter9))

```

### Convergence of PageRank

In this exercise, you will visually inspect how the PageRanks converge by plotting the differences of every two subsequent iterations.

- Create an empty vector called value.
- Write a for loop with 15 steps. For each step in the loop, compute the PageRank value of network with i iterations.
Add the vector attribute as a column to value using cbind().
- Compute the absolute difference between each subsequent pair of PageRank vectors in value and assign to difference.
- Plot the difference vector to inspect the convergence of the PageRank values.

```{r}

# Create an empty vector
value <- c()

# Write a loop to compute PageRank 
for(i in 1:15){
  value <- cbind(value, page_rank(network,options = list(niter = i))$vector)
}
  
# Compute the differences 
difference <- colSums(abs(value[,1:14] - value[,2:15]))

# Plot the differences
plot(1:14, difference)

```

### Personalized PageRank

In this exercise, you will study the difference between the PageRank and the personalized PageRank algorithms. You can use the function boxplots, which shows the score distributions of churners and non-churners with two separate boxplots. The function has two arguments:

damping, indicating the value of the damping factor. The default value is set to 0.85.
personalized, a Boolean parameter that indicates whether the personalized PageRank algorithm should be used. When TRUE, the restart vector has 1 for the churners in the network and 0 for the non-churners. The default value is FALSE, i.e. not personalized.


```{r}

boxplots <- function(damping = 0.85, personalized = FALSE) {
  if(personalized) {
    V(network)$pp <- page_rank(network, damping = damping, personalized = V(network)$churn)$vector
  }
  else {
    V(network)$pp <- page_rank(network, damping = damping)$vector
  }
  boxplot(V(network)$pp ~ V(network)$churn)
}

```


```{r}
# Look at the distribution of standard PageRank scores
boxplots(damping = 0.85)

# Inspect the distribution of personalized PageRank scores
boxplots(damping = 0.85, personalized = T)

# Look at the standard PageRank with damping factor 0.2
boxplots(damping = 0.2)

# Inspect the personalized PageRank scores with a damping factor 0.99
boxplots(0.99, T)
```

### Extract PageRank features

In this exercise, you will compute a few PageRank scores and add them as features to the network.

When personalizing, use the prior churners in the network as the restart vector. That is, include the argument personalized = V(network)$churn.

The default damping value is 0.85.

1. Compute the default PageRank score, extract the vector and add it as an attribute to network called pr_0.85.

```{r}

# Compute the default PageRank score
V(network)$pr_0.85 <- page_rank(network)$vector

```

2. Compute the PageRank score with damping factor 0.2 and add it to network as pr_0.20

```{r}

# Compute the PageRank score with damping 0.2
V(network)$pr_0.20 <- page.rank(network, damping = 0.2)$vector

```

3. Compute the default personalized PageRank score and call it perspr_0.85. Look at the instructions above to see how.

```{r}

# Compute the personalized PageRank score
V(network)$perspr_0.85 <- page.rank(network, personalized = V(network)$churn)$vector

```

4. Compute the personalized PageRank score with damping factor 0.99 and name it perspr_0.99.

```{r}

# Compute the personalized PageRank score with damping 0.99
V(network)$perspr_0.99 <- page_rank(network, damping = 0.99, personalized = V(network)$Churn)$vector

```

## Putting it all together


### Extract a dataset

### Getting a flat dataset

In this exercise, you will turn your network into a dataframe, where the rows are the people in the network and the columns are the network features you computed in the previous chapter. You will also prepare the dataset for the pre-processing.

- Extract the dataframe of the customers using the as_data_frame() function. Note that you want the node attributes, i.e. vertices. Call the dataset studentnetworkdata_full
- Inspect the first few rows of the data frame using the head() function.
- Remove the customers who already churned by conditioning on the Churn attribute. Call this dataframe studentnetworkdata_filtered
- Remove the first two columns, called Churn and name, since you don't need them anymore and name the dataframe studentnetworkdata.

```{r}

# Extract the dataset
studentnetworkdata_full <- igraph::as_data_frame(network, what = "vertices")

# Inspect the dataset
head(studentnetworkdata_full)

# Remove customers who already churned
studentnetworkdata_filtered <- studentnetworkdata_full[-which(studentnetworkdata_full$churn == 1), ]

# Remove useless columns
studentnetworkdata <- studentnetworkdata_filtered[, -c(1, 2)]

```

### Missing Values

You can check if a dataset has missing values per column using the apply() function.

- The first argument of apply() is the dataframe you are inspecting, in this case, studentnetworkdata.
- The second argument indicates whether operations should be performed row-wise (1) or column-wise (2).
- The third argument is the function applied to each row or column. You specify it by writing function(x) ... where ... indicates the operation.
You want to count the number of missing values in each column. Use sum(is.na(x)) to find all the missing values and count them.

How many columns in studentnetworkdata have missing values?


### Replace missing values

In the last exercise, you noticed that for six observations, the value of RelationalNeighborSecond was missing. In this exercise, you will replace those missing values with 0.

- Use summary() to inspect the RelationalNeighborSecond feature.
- Find the indices of the observations that are missing using which() and assign to the variable toReplace.
- Use the toReplace vector to replace the missing values instudentnetworkdata$RelationalNeighborSecond with a zero.
- Inspect RelationalNeighborSecond again to make sure there are no longer any missing value.

```{r}

# Inspect the feature
summary(studentnetworkdata$RelationalNeighbor2)

# Find the indices of the missing values
toReplace <- which(is.na(studentnetworkdata$RelationalNeighbor2))

# Replace the missing values with 0
studentnetworkdata$RelationalNeighbor2[toReplace] <- 0

# Inspect the feature again
summary(studentnetworkdata$RelationalNeighbor2)

```

### Correlated variables

In this exercise, you will inspect the dataset with respect to correlated variables. It is important to remove them before applying a binary classifier, especially in the case of logistic regression. When two or more variables are highly correlated you should remove all except for one.

First, we will use the corrplot() function in the corrplot package to visualize the correlations. In the correlation plot, blue represents a positive correlation and red a negative correlation. A darker color indicates a higher correlation. Finally, you will remove the highly correlated variables from the data set.

- Load the corrplot package.
- Generate a correlation matrix, M, using the function cor(). The function takes a subset of the dataset as an argument.
- Visualize the correlation between the variables using corrplot() and M.


```{r}

# Remove the Future column from studentnetworkdata 
no_future <- studentnetworkdata[, -1]

# Load the corrplot package
library(corrplot)

# Generate the correlation matrix
M <- cor(no_future)

# Plot the correlations
corrplot(M, method = "circle")

```

- Based on the plot, you should remove NonChurnNeighbors, NonChurnNeighborsSecond, pr_0.85, perspr_0.99.
- With colnames(), print the column names.
- Store the indices of the variables above toRemove.
- Remove them from studentnetworkdata.

```{r}

# Print the column names
names(studentnetworkdata)

# Create toRemove
toRemove <- c(10, 11, 19, 22)

# Remove the columns
studentnetworkdata_no_corrs <- studentnetworkdata[, -toRemove]



```

### Building a predictive model

### Split into train and test

Now that we have a dataframe, we can apply standard techniques for modeling. In this exercise, you will split the data into training and test sets.

- To ensure the reproducibility of your results, set a seed to 7, using set.seed().
- Use the sample() function to sample two-thirds of the numbers from the sequence from the range of the total number of rows in studentnetworkdata. Name this vector index_train.
- Create the training set by including the rows of studentnetworkdata that are stored in index_train and name it training_set.
- Create the test set by excluding the rows of studentnetworkdata that are stored in index_train and name it test_set.

```{r}

# Set the seed
set.seed(7)

# Creat the index vector
index_train <- sample(1:nrow(studentnetworkdata), 2 / 3 * nrow(studentnetworkdata))

# Make the training set
training_set <- studentnetworkdata[index_train,]

# Make the test set
test_set <- studentnetworkdata[-index_train,]

```

### Logistic regression model

In this exercise, you will build churn prediction models using logistic regression. These models predict which customers will churn in the future. You will build three models with different sets of features.
The target variable is called Future. You will build the models using the training dataset training_set and the function glm().

### Random forest model

In this exercise, you will use the randomForest() function in the randomForest package to build a random forest model for predicting churn of the customers in the training data set, training_set. The target variable is called Future. You will also inspect and visualize the importance of the variables in the model.


### Evaluating model performance

Predicting churn

In this exercise, you will use the predict() function in the pROC package to predict the churn probability of the customers in the test set, test_set. The function has three arguments:

- The model used to make the predictions.
- newdata: The dataset the model should be applied to.
- type: the type of prediction. We want to know the probability of churn, so we choose either response or prob.
Use the function predict() to predict the churn probability of the customers in test_set.

### Measure AUC

In this exercise, you will compute the AUC of your churn prediction models to find the best one. Use the auc() function in the pROC package. The function has two arguments:

The true churn label in the test set, test_set$Future.
The model prediction:
a. For logistic regression, it is the prediction obtained from the predict function.
b. For random forest, it is the second column of the prediction obtained from the predict function.
The objects firstPredictions, secondPredictions, thirdPredictions, and rfPredictions have been loaded for you.

Which model has the highest AUC value?


### Measure top decile lift

Compute and compare the top decile lift of the churn prediction models using the function TopDecileLift() in the lift package. The function takes two arguments:

- The true churn label in the test set, test_set$Future.
- The model prediction: a. For logistic regression, use the prediction from the predict function. b. For random forest, use the second column of the predictions from the predict function.
The objects firstPredictions, secondPredictions, thirdPredictions, and rfPredictions, and the package lifthave been loaded for you.

Which model has the highest top decile lift?








































