---
title: 'Supervised Learning in R: Classification'
author: "Yifei Liu"
date: "11/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(class)
library(naivebayes)
library(pROC)
library(rpart)
library(rpart.plot)
library(randomForest)
detach("package:dplyr", unload = TRUE)
library(dplyr)

signs <- read_csv("/Users/yifeiliu/Documents/R/data/EDA/datacamp/machine_learning/knn_traffic_signs.csv") %>%
  select(-id, -sample)

locations <- read_csv("/Users/yifeiliu/Documents/R/data/EDA/datacamp/machine_learning/locations.csv") 

donors <- read_csv("/Users/yifeiliu/Documents/R/data/EDA/datacamp/machine_learning/donors.csv") 

loans <- read_csv("/Users/yifeiliu/Documents/R/data/EDA/datacamp/machine_learning/loans.csv") %>%
  mutate(default = factor(default, levels = c(0, 1), labels = c("repaid", "default"))) %>%
  mutate_if(is.character, as.factor)
  
```


### Recognizing a road sign with kNN

After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.

As it begins to drive away, its camera captures the following image:

![](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_stop_99.gif)

Can you apply a kNN classifier to help the car recognize this sign?

The dataset signs is loaded in your workspace along with the dataframe next_sign, which holds the observation you want to classify.

```{r}
# Create a vector of labels

train_id <- 1:205

knn_predict <- knn(train = signs[train_id, ][-1], test = signs[-train_id, ][-1], signs$sign_type[train_id])



```

### Exploring the traffic sign dataset

To better understand how the knn() function was able to classify the stop sign, it may help to examine the training dataset it used.

Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

![](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_sign_data.png)


The result is a dataset that records the sign_type as well as 16 x 3 = 48 color properties of each sign.


```{r}
# Examine the structure of the signs dataset
str(signs)

# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)

signs %>%
  group_by(sign_type) %>%
  summarize(r10_mean = mean(r10))
```


### Classifying a collection of road signs

Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.

The test course includes 58 additional road signs divided into three types:
![](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_speed_55.gif)

At the conclusion of the trial, you are asked to measure the car's overall performance at recognizing these signs.

The class package and the dataset signs are already loaded in your workspace. So is the dataframe test_signs, which holds a set of observations you'll test your model on.

```{r}
train_id <- sample(1:nrow(signs), floor(nrow(signs)*0.8))

signs_train <- signs[train_id, ]
signs_type <- pull(signs_train, sign_type)
signs_test <- signs[-train_id, ]

signs_predict <- knn(train = signs_train[-1], signs_test[-1], cl = signs_type)

# Create a confusion matrix of the predicted versus actual values
signs_actual <- pull(signs_test, sign_type)
table(signs_predict, signs_actual)

# Compute the accuracy
mean(signs_predict == signs_actual)

```

### What about the 'k' in kNN?

### Understanding the impact of 'k'
There is a complex relationship between k and classification accuracy. Bigger is not always better.
Which of these is a valid reason for keeping k as small as possible (but no smaller)?

- A smaller k may utilize more subtle patterns

Answer: Yes! With smaller neighborhoods, kNN can identify more subtle patterns in the data.

### Testing other 'k' values

By default, the knn() function in the class package uses only the single nearest neighbor.

Setting a k parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare k values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.

The class package is already loaded in your workspace along with the datasets signs, signs_test, and sign_types. The object signs_actual holds the true values of the signs.


```{r}

knn_mult <- function(k) {
   knn_res <- knn(train = signs_train[-1], signs_test[-1], cl = signs_type, k = k)
   accuracy = mean(pull(signs_test, sign_type) == knn_res)
   paste0("When K = ", k, " Accuracy is: ", scales::percent(accuracy))
}

mapply(knn_mult, c(1, 7, 15))

```

### Seeing how the neighbors voted

When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the knn() function.

The class package has already been loaded in your workspace along with the datasets signs, sign_types, and signs_test.


```{r}

knn_mult <- function(k, prob) {
  knn_res <- knn(train = signs_train[-1], signs_test[-1], cl = signs_type, k = k, prob = T)
  accuracy = mean( signs_test$sign_type== knn_res)
  prob = attr(knn_res, "prob")
  res <- tibble(
    pred = head(knn_res),
    actual = head(pull(signs_test, sign_type)),
    prob = head(prob),
    accuracy = rep(accuracy, 6),
    k = rep(k, 6)
  )
  if(prob == T) {
    res
  } else
    paste0("When K = ", k, " Accuracy is: ", scales::percent(accuracy))
}

map2(c(1, 7, 15), c(T, T, T), knn_mult)

```

### Data preparation for kNN

### Why normalize data?
Before applying kNN to a classification task, it is common practice to rescale the data using a technique like min-max normalization. What is the purpose of this step?

- To ensure all data elements may contribute equal shares to distance.


```{r}

normalized <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

summary(normalized(signs$r1))
summary(signs$r1)

```


## Naive Bayes

### Understanding Bayesian methods

### Computing probabilities

The where9am data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his location at 9am each day as well as whether the daytype was a weekend or weekday.

Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.

$$P(A|B) = \frac {P(A \& B)}{P(B)}$$

Calculations like these are the basis of the Naive Bayes destination prediction model you'll develop in later exercises.

```{r}

where9am <- locations %>%
  filter(hour == 9)

# Compute P(A) 
p_A <- nrow(subset(where9am, location == "office")) / nrow(where9am)

# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday")) / nrow(where9am)

# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office" & daytype == "weekday")) / nrow(where9am)

# Compute P(A | B) and print its value
p_A_given_B <- p_AB / p_B
p_A_given_B


```

### A simple Naive Bayes location model

The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday.

To see this finding in action, use the where9am data frame to build a Naive Bayes model on the same data.

You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?

The dataframe where9am is available in your workspace. This dataset contains information about Brett's location at 9am on different days.


```{r}
# Load the naivebayes package
library(naivebayes)

# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)

# Predict Thursday's 9am location
predict(locmodel, newdata = where9am %>% filter(weekday == "thursday", daytype == "weekday"))

# Predict Saturdays's 9am location
predict(locmodel, newdata = where9am %>% filter(weekday == "saturday", daytype == "weekend"))
```

### Examining "raw" probabilities

The naivebayes package offers several ways to peek inside a Naive Bayes model.

Typing the name of the model object provides the a priori (overall) and conditional probabilities of each of the model's predictors. If one were so inclined, you might use these for calculating posterior (predicted) probabilities by hand.

Alternatively, R will compute the posterior probabilities for you if the type = "prob" parameter is supplied to the predict() function.

Using these methods, examine how the model's predicted 9am location probability varies from day-to-day. The model locmodel that you fit in the previous exercise is in your workspace.

```{r}

locmodel

# Obtain the predicted probabilityes for Thursday at 9am
predict(locmodel, newdata = where9am %>% filter(weekday == "thursday", daytype == "weekday"))

# Obtain the predicted probabilityes for Saturday at 9am
predict(locmodel, newdata = where9am %>% filter(weekday == "saturday", daytype == "weekend"))

```

### Understanding independence

Understanding the idea of event independence will become important as you learn more about how "naive" Bayes got its name. Which of the following is true about independent events?

- Knowing the outcome of one event does not help predict the other.

### Understanding NB's "naivety"

### Who are you calling naive?
The Naive Bayes algorithm got its name because it makes a "naive" assumption about event independence.

What is the purpose of making this assumption?

- The joint probability calculation is simpler for independent events.


### A more sophisticated location model

The locations dataset records Brett's location every hour for 13 weeks. Each hour, the tracking information includes the daytype (weekend or weekday) as well as the hourtype (morning, afternoon, evening, or night).

Using this data, build a more sophisticated model to see how Brett's predicted location not only varies by the day of week but also by the time of day. The dataset locations is already loaded in your workspace.

```{r}
# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, data = locations)

```

### Preparing for unforeseen circumstances

While Brett was tracking his location over 13 weeks, he never went into the office during the weekend. Consequently, the joint probability of P(office and weekend) = 0.

Explore how this impacts the predicted probability that Brett may go to work on the weekend in the future. Additionally, you can see how using the Laplace correction will allow a small chance for these types of unforeseen circumstances.

The model locmodel is already in your workspace, along with the dataframe weekend_afternoon.

```{r}

# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, newdata = locations[84, ], type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, data = locations, laplace = 1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, newdata = locations[84, ], type = "prob")

```

### Understanding the Laplace correction
By default, the naive_bayes() function in the naivebayes package does not use the Laplace correction. What is the risk of leaving this parameter unset?

- Some potential outcomes may be predicted to be impossible.

The small probability added to every outcome ensures that they are all possible even if never previously observed.

### Applying Naive Bayes to other problems

### Handling numeric predictors
Numeric data is often binned before it is used with Naive Bayes. Which of these is not an example of binning?


## Logistic Regression

### Making binary predictions with regression

### Building simple logistic regression models

The donors dataset contains 93,462 examples of people mailed in a fundraising solicitation for paralyzed military veterans. The donated column is 1 if the person made a donation in response to the mailing and 0 otherwise. This binary outcome will be the dependent variable for the logistic regression model.

The remaining columns are features of the prospective donors that may influence their donation behavior. These are the model's independent variables.

When building a regression model, it is often helpful to form a hypothesis about which independent variables will be predictive of the dependent variable. The bad_address column, which is set to 1 for an invalid mailing address and 0 otherwise, seems like it might reduce the chances of a donation. Similarly, one might suspect that religious interest (interest_religion) and interest in veterans affairs (interest_veterans) would be associated with greater charitable giving.

In this exercise, you will use these three factors to create a simple model of donation behavior. The dataset donors is available in your workspace.

```{r}
# Examine the dataset to identify potential independent variables
str(donors)

# Explore the dependent variable
table(donors$donated)

# Build the donation model
donation_model <- glm(donated ~ bad_address + interest_religion + interest_veterans, 
                      data = donors, family = "binomial")

# Summarize the model results
summary(donation_model)
```

### Making a binary prediction

In the previous exercise, you used the glm() function to build a logistic regression model of donor behavior. As with many of R's machine learning methods, you can apply the predict() function to the model object to forecast future behavior. By default, predict() outputs predictions in terms of log odds unless type = "response" is specified. This converts the log odds to probabilities.

Because a logistic regression model estimates the probability of the outcome, it is up to you to determine the threshold at which the probability implies action. One must balance the extremes of being too cautious versus being too aggressive. For example, if you were to solicit only the people with a 99% or greater donation probability, you may miss out on many people with lower estimated probabilities that still choose to donate. This balance is particularly important to consider for severely imbalanced outcomes, such as in this dataset where donations are relatively rare.

The dataset donors and the model donation_model are already loaded in your workspace.

```{r}

# Estimate the donation probability
donors$donation_prob <- predict(donation_model, type = "response")

# Find the donation probability of the average prospect
mean(donors$donated)

# Predict a donation if probability of donation is greater than average (0.0504)
donors$donation_pred <- ifelse(donors$donation_prob > 0.0504, 1, 0)

# Calculate the model's accuracy
mean(donors$donation_pred == donors$donated)



```

### The limitations of accuracy

In the previous exercise, you found that the logistic regression model made a correct prediction nearly 80% of the time. Despite this relatively high accuracy, the result is misleading due to the rarity of outcome being predicted.

The donors dataset is available in your workspace. What would the accuracy have been if a model had simply predicted "no donation" for each person?


### Model performance tradeoffs

### Calculating ROC Curves and AUC

The previous exercises have demonstrated that accuracy is a very misleading measure of model performance on imbalanced datasets. Graphing the model's performance better illustrates the tradeoff between a model that is overly agressive and one that is overly passive.

In this exercise you will create a ROC curve and compute the area under the curve (AUC) to evaluate the logistic regression model of donations you built earlier.

The dataset donors with the column of predicted probabilities, donation_prob ,is already loaded in your workspace.

- Load the pROC package.
- Create a ROC curve with roc() and the columns of actual and predicted donations. Store the result as ROC.
- Use plot() to draw the ROC object. Specify col = "blue" to color the curve blue.
- Compute the area under the curve with auc().



```{r}

# Create a ROC curve
ROC <- roc(donors$donated, donors$donation_pred)

# plot the ROC curve
plot(ROC, col = "blue")

# Calculate the area under the curve (AUC)
auc(ROC)

```


### Comparing ROC curves

Which of the following ROC curves illustrates the best model?

![](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/lr_auc_compare.png)


When AUC values are very close, it's important to know more about how the model will be used.

### Coding categorical features

Sometimes a dataset contains numeric values that represent a categorical feature.

In the donors dataset, wealth_rating uses numbers to indicate the donor's wealth level:

0 = Unknown
1 = Low
2 = Medium
3 = High
This exercise illustrates how to prepare this type of categorical feature and the examines its impact on a logistic regression model. The dataframe donors is loaded in your workspace.

- Create a factor from the numeric wealth_rating with labels as shown above by passing the factor() function the column you want to convert, the individual levels, and the labels.
- Use relevel() to change the reference category to Medium. The first argument should be your factor column.
- Build a logistic regression model using the column wealth_rating to predict donated and display the result with summary().

```{r}
# Convert the wealth rating to a factor
donors$wealth_rating <- factor(donors$wealth_rating, levels = c(0, 1, 2, 3), labels = c("Unknown", "Low", "Medium", "High"))

# Use relevel() to change reference category
donors$wealth_rating <- relevel(donors$wealth_rating, ref = "Medium")

# See how our factor coding impacts the model
summary(glm(donated ~ wealth_rating, data = donors, family = "binomial"))
```

### Handling missing data

Some of the prospective donors have missing age data. Unfortunately, R will exclude any cases with NA values when building a regression model.

One workaround is to replace, or impute, the missing values with an estimated value. After doing so, you may also create a missing data indicator to model the possibility that cases with missing data are different in some way from those without.

The dataframe donors is loaded in your workspace.

- Use summary() on donors$age to find the average age of prospects with non-missing data.
- Use ifelse() and the test is.na(donors$age) to impute the average (rounded to 2 decimal places) for cases with missing age. Be sure to also ignore NAs.
- Create a binary dummy variable named missing_age indicating the presence of missing data using another ifelse() call and the same test.


```{r}

# Find the average age among non-missing values
summary(donors$age)

# Impute missing age values with the mean age
donors$imputed_age <- ifelse(is.na(donors$age), round(mean(donors$age, na.rm = T), 2), donors$age)

# Create missing value indicator for age
donors$missing_age <- ifelse(is.na(donors$age), 1, 0)

```



### Understanding missing value indicators
A missing value indicator provides a reminder that, before imputation, there was a missing value present on the record.

Why is it often useful to include this indicator as a predictor in the model?

- A missing value may represent a unique category by itself

- There may be an important difference between records with and without missing data

- Whatever caused the missing value may also be related to the outcome


### Building a more sophisticated model

One of the best predictors of future giving is a history of recent, frequent, and large gifts. In marketing terms, this is known as R/F/M:

Recency
Frequency
Money
Donors that haven't given both recently and frequently may be especially likely to give again; in other words, the combined impact of recency and frequency may be greater than the sum of the separate effects.

Because these predictors together have a greater impact on the dependent variable, their joint effect must be modeled as an interaction. The donors dataset has been loaded for you.

- Create a logistic regression model of donated as a function of money plus the interaction of recency and frequency. Use * to add the interaction term.
- Examine the model's summary() to confirm the interaction effect was added.
- Save the model's predicted probabilities as rfm_prob. Use the predict() function, and remember to set the type argument.
- Plot a ROC curve by using the function roc(). Remember, this function takes the column of outcomes and the vector of predictions.
Compute the AUC for the new model with the function auc() and compare performance to the simpler model.


```{r}
# Build a recency, frequency, and money (RFM) model
rfm_model <- glm(donated ~ recency * frequency + money, data = donors, family = "binomial")

# Summarize the RFM model to see how the parameters were coded
summary(rfm_model)

# Compute predicted probabilities for the RFM model
rfm_prob <- predict(rfm_model, data = donors, type = "response")

# Plot the ROC curve for the new model
library(pROC)
ROC <- roc(donors$donated, rfm_prob)
plot(ROC, col = "red")
auc(ROC)
```

### Automatic feature selection

### The dangers of stepwise regression
In spite of its utility for feature selection, stepwise regression is not frequently used in disciplines outside of machine learning due to some important caveats. Which of these is one of these concerns?

- It is not guaranteed to find the best possible model
- The stepwise regression procedure violates some statistical assumptions
- It can result in a model that makes little sense in the real world

### Building a stepwise regression model

In the absence of subject-matter expertise, stepwise regression can assist with the search for the most important predictors of the outcome of interest.

In this exercise, you will use a forward stepwise approach to add predictors to the model one-by-one until no additional benefit is seen. The donors dataset has been loaded for you.

- Use the R formula interface with glm() to specify the base model with no predictors. Set the explanatory variable equal to 1.
- Use the R formula interface again with glm() to specify the model with all predictors.
- Apply step() to these models to perform forward stepwise regression. Set the first argument to null_model and set direction = "forward". This might take a while (up to 10 or 15 seconds) as your computer has to fit quite a few different models to perform stepwise selection.
- Create a vector of predicted probabilities using the predict() function.
Plot the ROC curve with roc() and plot() and compute the AUC of the stepwise model with auc().


```{r}

# Specify a null model with no predictors
null_model <- glm(donated ~ 1, data = donors, family = "binomial")

# Specify the full model using all of the potential predictors
full_model <- glm(donated ~ ., data = donors, family = "binomial")

# Use a forward stepwise algorithm to build a parsimonious model
step_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")

# Estimate the stepwise donation probability
step_prob <- predict(step_model, donors, type= "response")

# Plot the ROC of the stepwise model
library(pROC)
ROC <- roc(donors$donated, step_prob)
plot(ROC, col = "red")
auc(ROC)

```
Thought: In this case which use stepwise algorithms to choose model, no matter the direction, I negelect the interaction. [Adding interaction terms to step AIC in R](https://stackoverflow.com/questions/22418116/adding-interaction-terms-to-step-aic-in-r). The method could put interaction into consideration 



## Classification Trees

### Making decisions with trees

### Building a simple decision tree

The loans dataset contains 11,312 randomly-selected people who applied for and later received loans from Lending Club, a US-based peer-to-peer lending company.

You will use a decision tree to try to learn patterns in the outcome of these loans (either repaid or default) based on the requested loan amount and credit score at the time of application.

Then, see how the tree's predictions differ for an applicant with good credit versus one with bad credit.

The dataset loans is already in your workspace.

- Load the rpart package.
- Fit a decision tree model with the function rpart().
  - Supply the R formula that specifies outcome as a function of loan_amount and credit_score as the first argument.
  - Leave the control argument alone for now. (You'll learn more about that later!)
- Use predict() with the resulting loan model to predict the outcome for the good_credit applicant. Use the type argument to predict the "class" of the outcome.
- Do the same for the bad_credit applicant.

```{r}
set.seed(2019)

loans_small <- loans %>%
  filter(default == "default") %>%
  rbind(loans %>% filter(default == "repaid") %>% sample_n(5654))

# Build a lending model predicting loan outcome versus loan amount and credit score
loan_model <- rpart(default ~ loan_amount + credit_score, data = loans_small, method = "class", control = rpart.control(cp = 0))

# Make a prediction for someone with good credit
predict(loan_model, sample_n(loans_small %>% filter(credit_score == "HIGH"), 1), type = "class")

# Make a prediction for someone with bad credit
predict(loan_model, sample_n(loans_small %>% filter(credit_score == "LOW"), 1), type = "class")

```

### Visualizing classification trees

Due to government rules to prevent illegal discrimination, lenders are required to explain why a loan application was rejected.

The structure of classification trees can be depicted visually, which helps to understand how the tree makes its decisions. The model loan_model that you fit in the last exercise is in your workspace.


```{r}

# Examine the loan_model object
loan_model

# Plot the loan_model with default settings
rpart.plot(loan_model)

# Plot the loan_model with customized settings
rpart.plot(loan_model, type = 3, box.palette = c("red", "green"), fallen.leaves = TRUE)

```


### Growing larger classification trees

### Why do some branches split?
A classification tree grows using a divide-and-conquer process. Each time the tree grows larger, it splits groups of data into smaller subgroups, creating new branches in the tree.

Given a dataset to divide-and-conquer, which groups would the algorithm prioritize to split first?

- The group it can split to create the greatest improvement in subgroup homogeneity.



### Creating random test datasets

Before building a more sophisticated lending model, it is important to hold out a portion of the loan data to simulate how well it will predict the outcomes of future loan applicants.

As depicted in the following image, you can use 75% of the observations for training and 25% for testing the model.

![](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/dtree_test_set.png)

The sample() function can be used to generate a random sample of rows to include in the training set. Simply supply it the total number of observations and the number needed for training.

Use the resulting vector of row IDs to subset the loans into training and testing datasets. The dataset loans is loaded in your workspace.

```{r}
# Determine the number of rows for training
nrow(loans_small)

# Create a random sample of row IDs
sample_rows <- sample(nrow(loans_small), nrow(loans_small) * 0.75)

# Create the training dataset
loans_train <- loans_small[sample_rows, ]

# Create the test dataset
loans_test <- loans_small[-sample_rows, ]
```


### Building and evaluating a larger tree

Previously, you created a simple decision tree that used the applicant's credit score and requested loan amount to predict the loan outcome.

Lending Club has additional information about the applicants, such as home ownership status, length of employment, loan purpose, and past bankruptcies, that may be useful for making more accurate predictions.

Using all of the available applicant data, build a more sophisticated lending model using the random training dataset created previously. Then, use this model to make predictions on the testing dataset to estimate the performance of the model on future loan applications.

The rpart package is loaded into the workspace and the loans_train and loans_test datasets have been created.

- Use rpart() to build a loan model using the training dataset and all of the available predictors. Again, leave the control argument alone.
- Applying the predict() function to the testing dataset, create a vector of predicted outcomes. Don't forget the type argument.
- Create a table() to compare the predicted values to the actual outcome values.
- Compute the accuracy of the predictions using the mean() function.

```{r}

# Grow a tree using all of the available applicant data
loan_model <- rpart(default ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0))

# Make predictions on the test dataset
loans_test$pred <- predict(loan_model, loans_test, type = "class")

# Examine the confusion matrix
table(loans_test$default, loans_test$pred)

# Compute the accuracy on the test dataset
mean(loans_test$default == loans_test$pred)

```


### Conducting a fair performance evaluation

Holding out test data reduces the amount of data available for growing the decision tree. In spite of this, it is very important to evaluate decision trees on data it has not seen before.

Which of these is NOT true about the evaluation of decision tree performance?

- Decision trees sometimes overfit the training data.

- The model's accuracy is unaffected by the rarity of the outcome.

- Performance on the training dataset can overestimate performance on future data.

- Creating a test dataset simulates the model's performance on unseen data.

Answer: The model's accuracy is unaffected by the rarity of the outcome. 
Explain: _Right! Rare events cause problems for many machine learning approaches._

### Tending to classification trees

### Preventing overgrown trees

The tree grown on the full set of applicant data grew to be extremely large and extremely complex, with hundreds of splits and leaf nodes containing only a handful of applicants. This tree would be almost impossible for a loan officer to interpret.

Using the pre-pruning methods for early stopping, you can prevent a tree from growing too large and complex. See how the rpart control options for maximum tree depth and minimum split count impact the resulting tree.


- Use rpart() to build a loan model using the training dataset and all of the available predictors.
- Set the model controls using rpart.control() with parameters cp set to 0 and maxdepth set to 6.
- See how the test set accuracy of the simpler model compares to the original accuracy of 58.3%.
- First create a vector of predictions using the predict() function.
- Compare the predictions to the actual outcomes and use mean() to calculate the accuracy.

```{r}
# Grow a tree with maxdepth of 6
loan_model <- rpart(default ~., data = loans_train, method = "class", control = rpart.control(cp = 0, maxdepth = 6))

# Make a class prediction on the test set
loans_test$pred <- predict(loan_model, loans_test, type = "class")

# Compute the accuracy of the simpler tree
mean(loans_test$default == loans_test$pred)
```



- In the model controls, remove maxdepth and add a minimum split parameter, minsplit, set to 500.

```{r}

# Swap maxdepth for a minimum split of 500 
loan_model <- rpart(default ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0, minsplit = 500))

# Run this. How does the accuracy change?
loans_test$pred <- predict(loan_model, loans_test, type = "class")
mean(loans_test$pred == loans_test$default)

```

Result:  It may seem surprising, but creating a simpler decision tree may actually result in greater performance on the test dataset.


### Creating a nicely pruned tree

Stopping a tree from growing all the way can lead it to ignore some aspects of the data or miss important trends it may have discovered later.

By using post-pruning, you can intentionally grow a large and complex tree then prune it to be smaller and more efficient later on.

In this exercise, you will have the opportunity to construct a visualization of the tree's performance versus complexity, and use this information to prune the tree to an appropriate level.

The rpart package is loaded into the workspace, along with loans_test and loans_train.

- Use all of the applicant variables and no pre-pruning to create an overly complex tree. Make sure to set cp = 0 in rpart.control() to prevent pre-pruning.
- Create a complexity plot by using plotcp() on the model.
- Based on the complexity plot, prune the tree to a complexity of 0.0014 using the prune() function with the tree and the complexity parameter.
- Compare the accuracy of the pruned tree to the original accuracy of 58.3%. To calculate the accuracy use the predict() and mean() functions.

```{r}

# grow an overly complex tree
loan_model <- rpart(default ~., data = loans_train, method = "class", control = rpart.control(cp = 0))

# Examine the complexity plot
plotcp(loan_model)

# Prune the tree
loan_model_pruned <- prune(loan_model, cp = 0.0018)

# Compute the accuracy of the pruned tree
loans_test$pred <- predict(loan_model_pruned, loans_test, type = "class")
mean(loans_test$pred == loans_test$default)

```

### Why do trees benefit from pruning?
Classification trees can grow indefinitely, until they are told to stop or run out of data to divide-and-conquer.

Just like trees in nature, classification trees that grow overly large can require pruning to reduce the excess growth. However, this generally results in a tree that classifies fewer training examples correctly.

Why, then, are pre-pruning and post-pruning almost always used?

- Simpler trees are easier to interpret

- Simpler trees using early stopping are faster to train

- Simpler trees may perform better on the testing data

### Seeing the forest from the trees

### Understanding random forests
Groups of classification trees can be combined into an ensemble that generates a single prediction by allowing the trees to "vote" on the outcome.

Why might someone think that this could result in more accurate predictions than a single tree?

- The diversity among the trees may lead it to discover more subtle patterns.

Explain: The teamwork-based approach of the random forest may help it find important trends a single tree may miss.

### Building a random forest model

In spite of the fact that a forest can contain hundreds of trees, growing a decision tree forest is perhaps even easier than creating a single highly-tuned tree.

Using the randomForest package, build a random forest and see how it compares to the single trees you built previously.

Keep in mind that due to the random nature of the forest, the results may vary slightly each time you create the forest.

- Load the randomForest package.
- Build a random forest model using all of the loan application variables. The randomForest function also uses the formula interface.
- Compute the accuracy of the random forest model to compare to the original tree's accuracy of 58.3% using predict() and mean().

```{r}
# Build a random forest model
loan_model <- randomForest(default ~., data = loans_train)

# Compute the accuracy of the random forest
loans_test$pred <- predict(loan_model, loans_test, type = "response")
mean(loans_test$pred == loans_test$default)


```

This result is higher than our previous simple classification trees model. 





















